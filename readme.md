## WiTUnet: A New U-Shaped Architecture Integrating CNN and Transformer for Improved Feature Alignment and Fuse local and non-local Information.

<b>
Bin Wang, 
<a href='https://dengfei-ailab.github.io'>Deng Fei</a>, 
<a href='https://github.com/jiangpeifan'>Peifan Jiang</a>
</b>

<hr>
<i>Low-dose computed tomography (LDCT) has emerged as the preferred technology for diagnostic medical imaging due to the potential health risks associated with X-ray radiation and conventional computed tomography (CT) techniques. While LDCT utilizes a lower radiation dose compared to standard CT, it results in increased image noise, which can impair the accuracy of diagnoses. To mitigate this issue, advanced deep learning-based LDCT denoising algorithms have been developed. These primarily utilize Convolutional Neural Networks (CNNs) or Transformer Networks and often employ the Unet architecture, which enhances image detail by integrating feature maps from the encoder and decoder via skip connections. However, existing methods focus excessively on the optimization of the encoder and decoder structures while overlooking potential enhancements to the Unet architecture itself. This oversight can be problematic due to significant differences in feature map characteristics between the encoder and decoder, where simple fusion strategies may hinder effective image reconstruction. In this paper, we introduce WiTUnet, a novel LDCT image denoising method that utilizes nested, dense skip pathway in place of traditional skip connections to improve feature integration. Additionally, to address the high computational demands of conventional Transformers on large images, WiTUnet incorporates a windowed Transformer structure that processes images in smaller, non-overlapping segments, significantly reducing computational load. Moreover, our approach includes a Local Image Perception Enhancement (LiPe) module within both the encoder and decoder to replace the standard multi-layer perceptron (MLP) in Transformers, thereby improving the capture and representation of local image features. Through extensive experimental comparisons, WiTUnet has demonstrated superior performance over existing methods in critical metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Root Mean Square Error (RMSE), significantly enhancing noise removal and image quality.</i>



---
- overall structure
![WiTUnet](./img/overall_structure.jpg)
- nested dense block
![nested dense block](./img/nested_dense_block.jpg)
- LiPe
![LiPe](./img/LiPe.jpg)


---

## Result
Res
![res](./img/res.jpg)
ANOVA
![anova](./img/anova.jpg)
---
## The datasets and parallel framework of this framework are based on huggingface datasets&&accelerate
In addition to the pytorch environment, the following additional libraries are required before you can run it
```shell
pip install datasets accelerate timm einops scikit-image
```

Between the first runs, a configuration file is generated by accelerate, which tells accelerate which GPUs are currently involved in the training.
```shell
accelerate config
```

![image-20240325213824686](./img/acc_config.png)

After configuring the accelerate config file, you can view the configuration via `accelerate env` and test the hardware via `accelerate test`.

The test script is as follows

![image-20240325214053185](./img/acc_env.png)

---
You can then run `train.py` to train on a single card.

For multi-card training please cmd call
```shell
accelerate launch --gpu_ids=all  /export/pychamProject/LDCT-Denoising/train.py
```

---
If you find the cmd call scripts inelegant, you can also configure pycharm
- Create a new Run/Debug Configuration.
- Set the ENV of the RUN
- Set the mode to module, and give the model name: `accelerate.commands.launch`.
- Set parameter `--gpu_ids=all` to set available GPUs, specify script path `/{your_path}/LDCT-Denoising/trian.py`.
![配置pycham](./img/pycham_config.png)

For more distributed training parameters see https://huggingface.co/docs/accelerate/v0.28.0/en/basic_tutorials/launch

